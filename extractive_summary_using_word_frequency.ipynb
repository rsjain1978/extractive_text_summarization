{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "id": "DXSM_RfaZgmg",
    "outputId": "85e183d7-bd8f-4b47-bbaf-4d905ac40ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: spacy in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy) (4.42.1)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy) (45.2.0.post20200210)\n",
      "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "13JV3HDif5qh",
    "outputId": "3ba91122-a3f4-4f02-d3c0-9a67de9b8e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textwrap3\n",
      "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
      "Installing collected packages: textwrap3\n",
      "Successfully installed textwrap3-0.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install textwrap3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "colab_type": "code",
    "id": "GCsVG_GWZtdn",
    "outputId": "5f754c82-997a-4a49-c3fd-295c4187a855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.42.1)\n",
      "Requirement already satisfied: setuptools in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (45.2.0.post20200210)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\machinelearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.2.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "JHGfIP3wZxWC",
    "outputId": "dd49efde-d5cf-40de-e2e8-b5f2befdf440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Loading compatibility table...\n",
      "\u001b[2K[+] Loaded compatibility table\n",
      "\u001b[1m\n",
      "====================== Installed models (spaCy v2.2.4) ======================\u001b[0m\n",
      "[i] spaCy installation:\n",
      "C:\\MachineLearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\spacy\n",
      "\n",
      "TYPE      NAME             MODEL            VERSION      \n",
      "package   en-core-web-lg   en_core_web_lg   2.2.5     [+]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCbwAOdhZzyk"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import en_core_web_lg\n",
    "\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFGM3RsJaIA_"
   },
   "outputs": [],
   "source": [
    "def top_sentence(text, limit=2):\n",
    "    keyword = []\n",
    "\n",
    "    #POS tags we are interested in\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "\n",
    "    #convert to lower case\n",
    "    doc = nlp(text.lower())\n",
    "\n",
    "    #iterate through tokens\n",
    "    for token in doc:\n",
    "\n",
    "        #strip out stop words and punctuations\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        \n",
    "        #if then token's POS is in the list we are interested in, store the hold in list\n",
    "        if(token.pos_ in pos_tag):\n",
    "            keyword.append(token.text)\n",
    "\n",
    "    #get frequency of each word\n",
    "    freq_word = Counter(keyword)\n",
    "\n",
    "    #get maximum frequency across all words in the document\n",
    "    max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "\n",
    "    #normalize frequency of each word by dividing it by max frequency\n",
    "    for w in freq_word:\n",
    "        freq_word[w] = (freq_word[w]/max_freq)\n",
    "\n",
    "    #calculate the strength of each sentence using the frequency of each word in that sentence\n",
    "    sentence_strength={}\n",
    "    for sent in doc.sents:\n",
    "      for word in sent:\n",
    "        if word.text in freq_word.keys():\n",
    "          if sent in sentence_strength.keys():\n",
    "            sentence_strength[sent]+=freq_word[word.text]\n",
    "          else:\n",
    "            sentence_strength[sent]=freq_word[word.text]\n",
    "        \n",
    "    #sort sentences basis their strength\n",
    "    sorted_by_strength_sentences = sorted(sentence_strength.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    #create a summary basis the number of top 'k' sentences to be picked\n",
    "    summary=[]\n",
    "    for i in range(len(sorted_by_strength_sentences)):\n",
    "      if (i<limit):\n",
    "        txt = str(sorted_by_strength_sentences[i][0])\n",
    "        summary.append(txt.capitalize())\n",
    "      i+=1\n",
    "    \n",
    "    return ' '.join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CAh9LqvaLS0"
   },
   "outputs": [],
   "source": [
    "input =\"Another essential requirement is to have secure and reliable systems and software. Trustworthy AI requires algorithms to be secure, reliable and robust enough to deal with errors or inconsistencies during all life-cycle phases of an AI system. This requirement is about ensuring cybersecurity. In practice, all vulnerabilities should be taken into account when building algorithms. This requires testing AI systems to understand and mitigate the risks of cyber-attacks and hacking. AI developers should put in place processes capable of assessing the safety risks involved, in case someone uses the AI system they are building for harmful purposes. For instance, if the system is compromised, it should be possible for human control to take over and abort the system. To tackle this important question, the EU applies a twofold approach: first, fostering cooperation between the AI community and the security community, and second, reflecting on how to modify the legal framework governing liabilities in the EU, and to go from a human-conduct-based liability regime to a more machine-based liability regime.\"\n",
    "summary = top_sentence(input, limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "6z-VtmTFgBof",
    "outputId": "9dadb779-2268-4651-bb5f-a6c4f68c7eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another essential requirement is to have secure and reliable systems and software. Trustworthy AI requires algorithms to be secure, reliable and robust enough to deal with errors or inconsistencies during all life-cycle phases of an AI system. This requirement is about ensuring cybersecurity. In practice, all vulnerabilities should be taken into account when building algorithms. This requires testing AI systems to understand and mitigate the risks of cyber-attacks and hacking. AI developers should put in place processes capable of assessing the safety risks involved, in case someone uses the AI system they are building for harmful purposes. For instance, if the system is compromised, it should be possible for human control to take over and abort the system. To tackle this important question, the EU applies a twofold approach: first, fostering cooperation between the AI community and the security community, and second, reflecting on how to modify the legal framework governing liabilities in the EU, and to go from a human-conduct-based liability regime to a more machine-based liability regime.\n",
      "-------\n",
      "To tackle this important question, the eu applies a twofold approach: first, fostering cooperation between the ai community and the security community, and second, reflecting on how to modify the legal framework governing liabilities in the eu, and to go from a human-conduct-based liability regime to a more machine-based liability regime. Trustworthy ai requires algorithms to be secure, reliable and robust enough to deal with errors or inconsistencies during all life-cycle phases of an ai system.\n"
     ]
    }
   ],
   "source": [
    "from textwrap3 import wrap\n",
    "\n",
    "print (input)\n",
    "print ('-------')\n",
    "print (summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtJ3wApdbznY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled26.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
